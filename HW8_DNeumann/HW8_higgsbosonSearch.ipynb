{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my higgsbosonSearch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS4S7msAPK_R"
      },
      "source": [
        "- Download the Higgs boson data from Kaggle (programmatically within the notebook)\n",
        "see how I did it in the Titanic Trees notebook https://github.com/fedhere/DSPS/blob/master/lab9/titanictree.ipynb\n",
        "\n",
        "find the correct API link here https://www.kaggle.com/c/higgs-boson/data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMuvMJgOnlYo"
      },
      "source": [
        "\n",
        "- Read in the trainind data. Split the provided training data into a training and a test set. \n",
        "The last 2 columns are what you want to predict: \"weight\" and \"label\".\n",
        "Remove them from the input data and create a separate variable label and a separate variable weight, which will be your target variables for, respectively, classification and regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QncFXxr7xogC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52eb248d-5940-4e36-d3f7-ae2ac49b16ea"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import pylab as pl\n",
        "%pylab inline\n",
        "pl.style.use(\"https://raw.githubusercontent.com/fedhere/DSPS_FBianco/master/fbb.mplstyle\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn5mcME1NvNI",
        "outputId": "ba42b3de-63c0-448d-efe7-bc9a29276979"
      },
      "source": [
        "pip install kaggle "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcuXgoWrNviv",
        "outputId": "aa95811b-d060-401b-cd99-eaa33eecd678"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeCe-KKeN6Cw",
        "outputId": "563e3a8e-e428-4ca0-ccbd-ace4498c64ec"
      },
      "source": [
        "cd gdrive/My\\ Drive/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjM0ioiRN9Jb"
      },
      "source": [
        "# * Drive is mounted (in my google drive now)\n",
        "\n",
        "# create a file kaggle.json by requesting and API key to kaggle (under account) \n",
        "# https://www.kaggle.com/docs/api\n",
        "# and upload it to your google drive (manuallym outside of the notebook). Upload it to the general drive foloder and make sure it is still called kaggle.json\n",
        "\n",
        "Change the permission on the file so that it is secure and cannot be seen used others. The update the environmental variables KAGGLE_USERNAME and KAGGLE_KEY based on the values on the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-ZVr9EhN7RI",
        "outputId": "128a7b0f-d83c-426b-ab5f-7c29a660f7ef"
      },
      "source": [
        "ls kaggle.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5Lfbx2OObAz",
        "outputId": "37925538-2a11-43ef-f462-0674ef8cff99"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!cd ~/.kaggle/\n",
        "!chmod 600 kaggle.json"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gONn5-c4P28u",
        "outputId": "f48642cc-d731-4f8e-864d-6c920375617a"
      },
      "source": [
        "!ls ~/.kaggle"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcJXep_vOdjP"
      },
      "source": [
        "envs = json.load(open(\"kaggle.json\", \"r\"))\n",
        "os.environ[\"dakotaneumann\"] = envs['username']\n",
        "os.environ[\"e391cc52996c1c3fef44dd792fe4277f\"] = envs['key']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiAegKjrOiUC",
        "outputId": "8128742d-b6c1-4677-ee73-74561ff25ad3"
      },
      "source": [
        "#check that it worked by listing the kaggle datasets\n",
        "!kaggle datasets list"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                         title                                              size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "gpreda/reddit-vaccine-myths                                 Reddit Vaccine Myths                              237KB  2021-11-21 16:54:23          16053  \n",
            "crowww/a-large-scale-fish-dataset                           A Large Scale Fish Dataset                          3GB  2021-04-28 17:03:01           9692  \n",
            "imsparsh/musicnet-dataset                                   MusicNet Dataset                                   22GB  2021-02-18 14:12:19           4435  \n",
            "dhruvildave/wikibooks-dataset                               Wikibooks Dataset                                   2GB  2021-10-22 10:48:21           3543  \n",
            "promptcloud/careerbuilder-job-listing-2020                  Careerbuilder Job Listing 2020                     42MB  2021-03-05 06:59:52           2726  \n",
            "fatiimaezzahra/famous-iconic-women                          Famous Iconic Women                               838MB  2021-02-28 14:56:00           1901  \n",
            "nickuzmenkov/nih-chest-xrays-tfrecords                      NIH Chest X-rays TFRecords                         11GB  2021-03-09 04:49:23           1622  \n",
            "mathurinache/twitter-edge-nodes                             Twitter Edge Nodes                                342MB  2021-03-08 06:43:04           1312  \n",
            "alsgroup/end-als                                            End ALS Kaggle Challenge                           12GB  2021-04-08 12:16:37           1103  \n",
            "simiotic/github-code-snippets                               GitHub Code Snippets                                7GB  2021-03-03 11:34:39            502  \n",
            "mathurinache/the-lj-speech-dataset                          The LJ Speech Dataset                               3GB  2021-02-15 09:19:54            484  \n",
            "coloradokb/dandelionimages                                  DandelionImages                                     4GB  2021-02-19 20:03:47           1022  \n",
            "imsparsh/accentdb-core-extended                             AccentDB - Core & Extended                          6GB  2021-02-17 14:22:54            178  \n",
            "stuartjames/lights                                          LightS: Light Specularity Dataset                  18GB  2021-02-18 14:32:26            205  \n",
            "landrykezebou/lvzhdr-tone-mapping-benchmark-dataset-tmonet  LVZ-HDR Tone Mapping Benchmark Dataset (TMO-Net)   24GB  2021-03-01 05:03:40            232  \n",
            "nickuzmenkov/ranzcr-clip-kfold-tfrecords                    RANZCR CLiP KFold TFRecords                         2GB  2021-02-21 13:29:51            161  \n",
            "datasnaek/youtube-new                                       Trending YouTube Video Statistics                 201MB  2019-06-03 00:56:47         158524  \n",
            "zynicide/wine-reviews                                       Wine Reviews                                       51MB  2017-11-27 17:08:04         148882  \n",
            "datasnaek/chess                                             Chess Game Dataset (Lichess)                        3MB  2017-09-04 03:09:09          25246  \n",
            "residentmario/ramen-ratings                                 Ramen Ratings                                      40KB  2018-01-11 16:04:39          30015  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Px73fFQOGP",
        "outputId": "7f692503-b201-4c0e-d643-1e72617304e4"
      },
      "source": [
        "!kaggle competitions download -c higgs-boson"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading test.zip to /content/gdrive/My Drive\n",
            " 72% 25.0M/34.8M [00:00<00:00, 121MB/s]\n",
            "100% 34.8M/34.8M [00:00<00:00, 115MB/s]\n",
            "Downloading random_submission.zip to /content/gdrive/My Drive\n",
            "  0% 0.00/2.58M [00:00<?, ?B/s]\n",
            "100% 2.58M/2.58M [00:00<00:00, 82.7MB/s]\n",
            "Downloading HiggsBosonCompetition_AMSMetric_rev1.py to /content/gdrive/My Drive\n",
            "  0% 0.00/3.15k [00:00<?, ?B/s]\n",
            "100% 3.15k/3.15k [00:00<00:00, 974kB/s]\n",
            "Downloading training.zip to /content/gdrive/My Drive\n",
            " 83% 14.0M/16.9M [00:00<00:00, 142MB/s]\n",
            "100% 16.9M/16.9M [00:00<00:00, 108MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36d2etyhQTot"
      },
      "source": [
        "# higgs boson downloaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "begrK336RqKp",
        "outputId": "dd641e92-aece-4e93-c21d-bd3383be3b41"
      },
      "source": [
        "!unzip -l training.zip"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  training.zip\n",
            "  Length      Date    Time    Name\n",
            "---------  ---------- -----   ----\n",
            " 55253673  2014-05-09 11:40   training.csv\n",
            "---------                     -------\n",
            " 55253673                     1 file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3c_3ZGgSt6U"
      },
      "source": [
        "higgsdata = pd.read_csv(\"training.csv\").dropna(subset=[\"Survived\", \"Sex\", \"Pclass\", \"Age\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muWGBHN90IPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "c5384ca5-68f7-45f3-d604-3a3b132058de"
      },
      "source": [
        "higgsdata.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-13cab6ff29bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhiggsdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'higgsdata' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCPS73k0ytqj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "92066e17-9199-46a7-b4e6-f1fd26fd21c7"
      },
      "source": [
        "higgsdata.describe()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2a703a907968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhiggsdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'higgsdata' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8eUWMdVx983"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv1otxd5yB-X"
      },
      "source": [
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_pC_3biq_xU"
      },
      "source": [
        "from sklearn.model_selection import train_test_split "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLK2qoS_oW_j"
      },
      "source": [
        "- Use a Random Forest and a Gradiend Boosted Tree Classifier model to predict the label of the particles. get the score of the model on the training and test set and comment on the result for each model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htf_qENzNNcV"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Look at parameters used by our current forest\n",
        "rf = RandomForestClassifier(random_state = 0)\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(rf.get_params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eNZHaTYy89c"
      },
      "source": [
        "gbt = GradientBoostingClassifier(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Mub5WOzDYU"
      },
      "source": [
        "rf.fit(...)\n",
        "gbt.fit(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C_3_nhoPhQY"
      },
      "source": [
        "calculate the  scores for the training and test sets and evaluate  overtraining etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz3A54YTqXw1"
      },
      "source": [
        "rf.score(..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aiULt4TP8kn"
      },
      "source": [
        "- Produce a confusion matrix for each model and compare them\n",
        "\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak3gnF6duviH"
      },
      "source": [
        "\n",
        "# I creaded this function (mostly copied from sklearn examples). \n",
        "# You can use it to create the confusion matrix\n",
        "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def plot_confusion_matrix(y_true, y_pred,\n",
        "                          normalize=False,\n",
        "                          title='',\n",
        "                          cmap=pl.cm.bone):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"         \n",
        "    if normalize:\n",
        "          title = title + ' Normalized confusion matrix'\n",
        "    else:\n",
        "          title = title + ' Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # plot it\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    fig.subplots_adjust()\n",
        "    im = ax.imshow(cm, cmap=cmap)\n",
        "    ax_divider = make_axes_locatable(ax)\n",
        "    # add an axes to the right of the main axes.\n",
        "    pl.xticks([0, 1], labels=[\"N\", \"P\"])\n",
        "    pl.ylim(-0.5,1.5)\n",
        "    pl.yticks([0,1], labels=[\"N\", \"P\"])    \n",
        "    pl.title(title)\n",
        "    cax = ax_divider.append_axes(\"right\", size=\"10%\", pad=\"2%\")\n",
        "    cb = colorbar(im, cax=cax)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isbiIC8qxEA6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgQcTMFuxZV4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbZQ3kMSQFK6"
      },
      "source": [
        "\n",
        "- Use a Random Forest and a Gradiend Boosted Tree Regressor model to predict the weight of the particles. Compare the model performance on training and test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_6uYj243Be_"
      },
      "source": [
        "traindata_weights, testdata_weights, train_weights, test_weights = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkanVFfO3HBe"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rfR = RandomForestRegressor(n_estimators=100, max_depth=3,\n",
        "                                  random_state=0)\n",
        "rfR.fit(...)\n",
        "gbtR = ...\n",
        "gbtR.fit(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKvyqcFr3Z3d"
      },
      "source": [
        " Calculate the L2 and L1 loss functions for the fitted regression models (see slides for the definition) and discuss the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAMTAgRTIeHq"
      },
      "source": [
        "\n",
        "print(\"L1 RF.....\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c38At7ayYd_5"
      },
      "source": [
        "- For the Random Forest classifier, find the 4 most important features based on the simple unoptimized model you created earlier on. Use the documentation to find out what they are. We have not talked abotu the physics of this problem at all but the Kaggle challenge description should provide enogh information for you to comment on this result is somewhat superficially.\n",
        "\n",
        "    You can use ```rf.feature_importance_``` on the trained model to extract the relative importance of each feature (a number from 0 to 1) and then choose the features that have the 4 highest numbers (the numpy function ```argsort()``` is helpful here!)\n",
        "\n",
        "- Explore the parameter space with the sklearn module ```sklearn.model_selection.RandomizedSearchCV``` *fitting only those 4 features*\n",
        "\n",
        "    Follow this example to set up the parameter search. Set the estimators to 10 and 100, (the number of trees) and the max depth to 3, and 10, and None (let it be unconstrained). Set bootstrap to both True and False. Set the number of features to consider at every split to both \"autp\" and \"sqrt\". Use ```pprint``` like I did earlier in this notebook to print the parameters set\n",
        "\n",
        "    **this takes some computational time! so do not start this at the last minute!!**\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh6kkO3yctVH"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "# Number of trees in random forest\n",
        "n_estimators = [10,100,1000]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [3,10,100]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "#min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "##min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               #'min_samples_split': min_samples_split,\n",
        "               #'min_samples_leaf': min_samples_leaf,\n",
        "               #'bootstrap': bootstrap\n",
        "               }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-QQqGlp5ty3"
      },
      "source": [
        "pprint(random_grid)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-tgWZsoddEV"
      },
      "source": [
        "## rerun the model with the best hyperparameter set "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfdKk4Pk7tTz"
      },
      "source": [
        " mine and your best features do not necessarily have to be the same because our models may be different (different parameters, different random seed etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgh8TOENZn6p"
      },
      "source": [
        "higgsdata.iloc[:, best_features_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AJ70oqZPvL_"
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 18 different combinations\n",
        "random_search = RandomizedSearchCV(estimator = rf, param_distributions=random_grid,\n",
        "                                   n_iter=1, cv=3, iid=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3W60KM1Jb5r"
      },
      "source": [
        "Note that this may take a long time! It took 1 hour for me to run this. Dont start at the last minute!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCzypqyrtsEg"
      },
      "source": [
        "random_search.fit(...\n",
        "                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GfLrtvPBVT7"
      },
      "source": [
        "print(\"Tuned RF  Parameters: {}\".format(...))\n",
        "print(\"Best score is {}\".format(...))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaHvlFa27EV4"
      },
      "source": [
        "df = pd.DataFrame(random_search.cv_results_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pcXfIzb7eEw"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUJw96TLdax6"
      },
      "source": [
        "random_search.best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giXP87LNdu7v"
      },
      "source": [
        "## Run the model with the best parameters found by the grid search. Did the model improve? did overfitting improve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1r9zN9yI-at"
      },
      "source": [
        "# Plot a ROC curve for the model with the best parameters found in the previous step. Describe it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl2NKYAy9LYt"
      },
      "source": [
        "rf.fit(traindata, train_label.values.flatten()==\"s\", max_depth.....)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap4ZcVFm7zyD"
      },
      "source": [
        "# The random forest model by itself\n",
        "from sklearn.metrics import roc_curve\n",
        "y_pred_grd_rfcat = rf.predict_proba(testdata)[:, 1]\n",
        "\n",
        "fpr_rf, tpr_rf, _ = roc_curve(test_label.values.flatten()==\"s\", rf.predict(testdata))\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jiROnM7-E0n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}